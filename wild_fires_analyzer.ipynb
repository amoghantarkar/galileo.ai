{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wild fires analyzer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmlEUaZ72TX9Oa//hcBz9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amoghantarkar/galileo.ai/blob/main/wild_fires_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_OhN3CBX3E-"
      },
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATASETS\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE CELL.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "\n",
        "CHUNK_SIZE = 40960 \n",
        "DATASET_MAPPING = 'australian-bush-fire-satellite-data-nasa:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F495798%2F920571%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20201012%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20201012T042520Z%26X-Goog-Expires%3D259199%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D62a925e60e142c6396bbb8da491eafaf2f032a6a24c46a9086c9c4f3846d4cffa640d1e92c624dcf9dabfd0aa04601c577acc9d910b8f5329b687b3442f4e595ef5a1d65ca859436eae778b2dfe0c2613479c632543c1c3afbf11699d1a564718e6e8a10f3129c2c10214cf094cec1076b92a91b7e668a3477d6370454560dfc497cb188c3f358d011c93fae1bb1783ae03f751cdabf585624ad1604b9107ce9dab1cb3a2344ddfbf6ecdf219f37695a7a7afba9e9cdc24c9d71979d24f0bc99982827c7cadd4b15d5df563a3ce83363b29667ddcd702969292c74401b585f88112d6af8f73690fdba16d348568edf42e7221e1c2ce281d151db955bbff70ac7,indian-wildfire-nasa-dataset-8-years:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F564667%2F1066790%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20201012%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20201012T042520Z%26X-Goog-Expires%3D259199%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D60f6b07cb4e4b70fb00c62e84e3400af3e7fb97b0c75d1110a504e37ee10f9fed710f73dbfb9bc290945b8f25c7eec245b116bcebb948e5071fefd35040f0475780486fc056e5c848f94ffca1b32a37ce1268d8a5233ce38d7bbfb7fd3d32e4aa8144d39930b71a6b6ebd37a6583f02747276ec2fccb97b8fa792dbd2351bf954f5d2f656650a3feec687b636ea47fe53588367d441ba731584d512409baeab43b28788600494be4514d42d72f01c8476c792d05f9862d25403b9e04c7837150875ea675d85a9815cc0aaa4cccb3c9a421084f3f75973f96eb480f2adf835291ff75edea1f627c872edb9c3267502bb2d3f135934f37ad015784c83730db1d50'\n",
        "KAGGLE_INPUT_PATH='/home/kaggle/input'\n",
        "KAGGLE_INPUT_SYMLINK='/kaggle'\n",
        "\n",
        "!mkdir -p -- $KAGGLE_INPUT_PATH\n",
        "!chmod 777 $KAGGLE_INPUT_PATH\n",
        "!ln -sfn $KAGGLE_INPUT_PATH ../\n",
        "!mkdir -p -- $KAGGLE_INPUT_SYMLINK\n",
        "!ln -sfn $KAGGLE_INPUT_PATH $KAGGLE_INPUT_SYMLINK\n",
        "\n",
        "for dataset_mapping in DATASET_MAPPING.split(','):\n",
        "    directory, download_url_encoded = dataset_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as zipfileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = zipfileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes zipped')\n",
        "            dl = 0\n",
        "            data = zipfileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = zipfileres.read(CHUNK_SIZE)\n",
        "            print(f'\\nUnzipping {directory}')\n",
        "            with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "print('Dataset import complete.')\n",
        "\n",
        "\n",
        "!pip install git+https://github.com/python-visualization/branca\n",
        "!pip install git+https://github.com/sknzl/folium@update-css-url-to-https\n",
        "\n",
        "#dependencies\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt #plotting\n",
        "import seaborn as sns #for beatiful visualization\n",
        "import folium\n",
        "from folium import plugins\n",
        "\n",
        "#set file path\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "\n",
        "\n",
        "fire_nrt_m6 = pd.read_csv(\"../input/indian-wildfire-nasa-dataset-8-years/fire_nrt_M6_107977.csv\")\n",
        "fire_archive_m6 = pd.read_csv(\"../input/indian-wildfire-nasa-dataset-8-years/fire_archive_M6_107977.csv\")\n",
        "fire_nrt_v1 = pd.read_csv(\"../input/indian-wildfire-nasa-dataset-8-years/fire_nrt_V1_107978.csv\")\n",
        "fire_archive_v1 = pd.read_csv(\"../input/indian-wildfire-nasa-dataset-8-years/fire_archive_V1_107978.csv\")\n",
        "fire_nrt_v2 = pd.read_csv(\"../input/australian-bush-fire-satellite-data-nasa/fire_nrt_V1_101674.csv\")\n",
        "fire_archive_v2 = pd.read_csv(\"../input/australian-bush-fire-satellite-data-nasa/fire_archive_V1_101674.csv\")\n",
        "\n",
        "# type(fire_nrt_v1)\n",
        "\n",
        "df_filter = data.filter([\"latitude\",\"longitude\",\"acq_date\",\"frp\"])\n",
        "df_filter.head()\n",
        "\n",
        "df = df_filter[df_filter['acq_date']>='2015-11-01']\n",
        "df.count()\n",
        "\n",
        "data_topaffected = df.sort_values(by='frp',ascending=False).head(200)\n",
        "data_topaffected\n",
        "\n",
        "\n",
        "#Create a map\n",
        "\n",
        "mapbox_api_key = r\"pk.eyJ1IjoiYW1vZ2hhbnRhcmthciIsImEiOiJja2c1cHJlMjgwMXd3Mnpxdm9lbGY4aHp5In0.KMFMRgjtKlU-CzMdb5IgAQ\" \n",
        "tileset_ID_str = \"satellite-streets-v11\";#mapbox://styles/mapbox/satellite-streets-v11\n",
        "tilesize_pixels = \"256\"; \n",
        "m = folium.Map(location=[28.644800, 77.216721], zoom_start=3, \n",
        "                tiles = f\"https://api.mapbox.com/styles/v1/mapbox/{tileset_ID_str}/tiles/{tilesize_pixels}/{{z}}/{{x}}/{{y}}@2x?access_token={mapbox_api_key}\", \n",
        "                attr=\"Mapbox\")\n",
        "# m = folium.Map(location=[28.644800, 77.216721], control_scale=True, zoom_start=2,attr = \"text some\")\n",
        "df_copy = data_topaffected.copy()\n",
        "\n",
        "# loop through data to create Marker for each hospital\n",
        "for i in range(0,len(df_copy)):\n",
        "    \n",
        "    folium.Marker(\n",
        "    location=[df_copy.iloc[i]['latitude'], df_copy.iloc[i]['longitude']],\n",
        "    #popup=popup,\n",
        "    tooltip=\"frp: \" + str(df_copy.iloc[i]['frp']) + \"<br/> date: \"+ str(df_copy.iloc[i]['acq_date']),\n",
        "    icon=folium.Icon(color='red',icon='fire',prefix=\"fa\"),\n",
        "    ).add_to(m)\n",
        "        \n",
        "m\n",
        "\n",
        "from folium.plugins import HeatMapWithTime\n",
        "# A small function to get heat map with time given the data\n",
        "\n",
        "def getmap(ip_data,location,zoom,radius):\n",
        "    \n",
        "    #get day list\n",
        "    dfmap = ip_data[['acq_date','latitude','longitude','frp']]\n",
        "    df_day_list = []\n",
        "    for day in dfmap.acq_date.sort_values().unique():\n",
        "        df_day_list.append(dfmap.loc[dfmap.acq_date == day, ['acq_date','latitude', 'longitude', 'frp']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist())\n",
        "    \n",
        "    mapbox_api_key = r\"pk.eyJ1IjoiYW1vZ2hhbnRhcmthciIsImEiOiJja2c1cHJlMjgwMXd3Mnpxdm9lbGY4aHp5In0.KMFMRgjtKlU-CzMdb5IgAQ\" \n",
        "    tileset_ID_str = \"satellite-streets-v11\";#mapbox://styles/mapbox/satellite-streets-v11\n",
        "    tilesize_pixels = \"256\"; \n",
        "    m = folium.Map(location, zoom_start=zoom, \n",
        "                     tiles = f\"https://api.mapbox.com/styles/v1/mapbox/{tileset_ID_str}/tiles/{tilesize_pixels}/{{z}}/{{x}}/{{y}}@2x?access_token={mapbox_api_key}\", \n",
        "                     attr=\"Mapbox\")\n",
        "    # Create a map using folium\n",
        "#     m = folium.Map(location, zoom_start=zoom,tiles='Stamen Terrain')\n",
        "    #creating heatmap with time\n",
        "    HeatMapWithTime(df_day_list,index =list(dfmap.acq_date.sort_values().unique()), auto_play=False,radius=radius, gradient={0.2: 'blue', 0.4: 'lime', 0.6: 'orange', 1: 'red'}, min_opacity=0.5, max_opacity=0.8, use_local_extrema=True).add_to(m)\n",
        "\n",
        "    return m\n",
        "m=getmap(df,[28.644800, 77.216721],3.5,2)\n",
        "m\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}